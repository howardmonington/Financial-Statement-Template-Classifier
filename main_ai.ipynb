{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# import sklearn libraries \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# import ml models\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# import deep learning tools and tokenizer / pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "My first step will be to pull in the data that I will be using and converting it into a format that I can try feeding into different models. To do this, I will need to convert the tags in the training data into numbers. I do this by creating a tokenizer which gives each tag a unique number. For the target, I use a simple label encoder to encode into 0 and 1. Next, I need all of the data to be the same length to feed into my ML and DL models, so I use pad_sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in training set, split into train and and target\n",
    "train = pd.read_csv('financial-statement-ml-challenge-train.csv')\n",
    "y = train['FINANCIAL_STATEMENT_TEMPLATE']\n",
    "train = train['COMPANY_XBRL_TAGS']\n",
    "\n",
    "# pull in test set\n",
    "test = pd.read_csv('financial-statement-ml-challenge-test.csv')\n",
    "test = test['COMPANY_XBRL_TAGS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AccumulatedOtherComprehensiveIncomeLossNetOfTax AdvertisingExpense AmortizationOfDebtDiscountPremium AmortizationOfIntangibleAssets Assets AvailableForSaleSecurities AvailableForSaleSecuritiesDebtSecurities BankOwnedLifeInsurance BankOwnedLifeInsuranceIncome BankOwnedLifeInsuranceIncomeIncludingIncomeFromDeathBenefits BrokerageCommissionsRevenue BrokeredTimeDeposits BusinessCombinationAcquisitionRelatedCosts BusinessCombinationContingentConsiderationArrangementsChangeInAmountOfContingentConsiderationLiability1 CashAcquiredFromAcquisition CashAndCashEquivalentsPeriodIncreaseDecrease CashAndDueFromBanks CashCashEquivalentsAndFederalFundsSold CashCashEquivalentsRestrictedCashAndRestrictedCashEquivalentsPeriodIncreaseDecreaseIncludingExchangeRateEffect CommitmentsAndContingencies CommonStockDividendsPerShareDeclared CommonStockValue CommunicationsAndInformationTechnology ComprehensiveIncomeNetOfTax CoreProcessingCharges DebtAndEquitySecuritiesGainLoss DebtSecuritiesGainLoss Deposits DepreciationAmortizationAndAccretionNet DividendIncomeOperating EarningsPerShareBasic EarningsPerShareBasicAndDiluted EarningsPerShareDiluted EarningsPerShareDilutedAndBasic EquityMethodInvestmentRealizedGainLossOnDisposal EquitySecuritiesFvNi EquitySecuritiesFvNiGainLoss EquitySecuritiesFvNiRealizedGainLoss FederalDepositInsuranceCorporationPremiumExpense FederalFundsSold FederalFundsSoldAndOther FederalFundsSoldAndOtherInterestAndDividendIncome FederalFundsSoldAndOtherInterestIncome FeesAndCommissionsDebitCards FeesAndCommissionsDepositorAccounts FeesAndCommissionsFiduciaryAndTrustActivities FiniteLivedIntangibleAssetsNet GainLossOnSaleOfEquityMethodInvestments GainLossOnSaleOfProperties GainLossOnSaleOfPropertyPlantEquipment GainLossOnSaleOfSecuritiesNet GainLossOnSalesOfLoansNet GainsLossesOnSalesOfAssets Goodwill ImpairmentOfInvestments ImprovementsToRealEstateOwned IncomeLossFromContinuingOperationsBeforeIncomeTaxesExtraordinaryItemsNoncontrollingInterest IncomeLossFromContinuingOperationsBeforeIncomeTaxesMinorityInterestAndIncomeLossFromEquityMethodInvestments IncomeRecognizedFromDeathBenefitOnBankOwnedLifeInsurance IncomeTaxesPaid IncomeTaxExpenseBenefit IncreaseDecreaseInDeposits IncreaseDecreaseInOtherOperatingCapitalNet IncreaseInBankOwnedLifeInsurance InformationTechnologyAndDataProcessing InsuranceCommissionsAndFees IntangibleAssetsNetExcludingGoodwill InterestAndDividendIncomeOperating InterestAndFeeIncomeLoansAndLeases InterestBearingDepositLiabilities InterestExpense InterestExpenseDeposits InterestExpenseLongTermDebt InterestExpenseShortTermBorrowings InterestIncomeExpenseAfterProvisionForLoanLoss InterestIncomeExpenseNet InterestIncomeFederalFundsSold InterestIncomeSecuritiesTaxable InterestIncomeSecuritiesTaxExempt InterestPaid InterestPaidNet IssuanceOfStockAwards LaborAndRelatedExpense Liabilities LiabilitiesAndStockholdersEquity LoanProcessingFee LoansAndLeasesReceivableAllowance LoansAndLeasesReceivableNetOfDeferredIncome LoansAndLeasesReceivableNetReportedAmount LoansReceivableHeldForSaleNet LoansReceivableHeldForSaleNetNotPartOfDisposalGroup LoansSoldIncome LongTermDebt MarketableSecuritiesEquitySecurities MergerRelatedCostsIncome NetAmortizationOfSecurities NetCashProvidedByUsedInFinancingActivities NetCashProvidedByUsedInFinancingActivitiesContinuingOperations NetCashProvidedByUsedInInvestingActivities NetCashProvidedByUsedInInvestingActivitiesContinuingOperations NetCashProvidedByUsedInOperatingActivities NetCashProvidedByUsedInOperatingActivitiesContinuingOperations NetIncomeLoss NetIncomePerShareBasicAndDiluted NonCashOrPartNonCashAcquisitionAcquisitionCost NonCashOrPartNonCashAcquisitionFairValueOfAssetsAcquired NonCashOrPartNonCashAcquisitionValueOfLiabilitiesAssumed NoninterestBearingDepositLiabilities NoninterestExpense NoninterestIncome NoninterestIncomeOtherOperatingIncome OccupancyNet OtherAssets OtherComprehensiveIncomeAvailableForSaleSecuritiesAdjustmentNetOfTaxPeriodIncreaseDecrease OtherLiabilities OtherNoninterestExpense PaymentsForOriginationOfMortgageLoansHeldForSale PaymentsForProceedsFromLifeInsurancePolicies PaymentsForProceedsFromLoansAndLeases PaymentsForRepurchaseOfCommonStock PaymentsOfDividendsCommonStock PaymentsToAcquireAvailableForSaleSecurities PaymentsToAcquireAvailableForSaleSecuritiesDebt PaymentsToAcquireBusinessesAndInterestInAffiliates PaymentsToAcquireBusinessesNetOfCashAcquired PaymentsToAcquireFederalReserveStock PaymentsToAcquireLifeInsurancePolicies PaymentsToAcquirePropertyPlantAndEquipment PrepaymentFeesOnAdvances ProceedsFromDividendReinvestment ProceedsFromFederalHomeLoanBankBorrowings ProceedsFromInsuranceSettlementInvestingActivities ProceedsFromIssuanceOfCommonStock ProceedsFromIssuanceOfLongTermDebt ProceedsFromIssuanceOfLongTermDebtAndCapitalSecuritiesNet ProceedsFromLifeInsurancePolicies ProceedsFromMaturitiesPrepaymentsAndCallsOfAvailableForSaleSecurities ProceedsFromRepaymentsOfLongTermDebtAndCapitalSecurities ProceedsFromRepaymentsOfShortTermDebt ProceedsFromSaleOfAvailableForSaleSecurities ProceedsFromSaleOfAvailableForSaleSecuritiesDebt ProceedsFromSaleOfEquitySecurities ProceedsFromSaleOfFederalReserveStock ProceedsFromSaleOfLandHeldForUse ProceedsFromSaleOfLoansHeldForSale ProceedsFromSaleOfProductiveAssets ProceedsFromSaleOfPropertyHeldForSale ProceedsFromSaleOfRealEstate ProceedsFromSaleOfRestrictedInvestments ProceedsFromSaleOfTreasuryStock ProceedsFromSaleOfWhollyOwnedRealEstateAndRealEstateAcquiredInSettlementOfLoans ProfessionalFees PropertyPlantAndEquipmentNet ProvisionForLoanLossesExpensed ProvisionReversalForLoanLossesExpensed PurchaseOfBankOwnedLifeInsurance PurchaseOfEquitySecurities PurchaseOfRestrictedStock RepaymentsOfLongTermDebt RepaymentsOfLongTermDebtAndCapitalSecurities RetainedEarningsAccumulatedDeficit RetirementPlanConsultingFees RevenueFromContractWithCustomerExcludingAssessedTax SecurityPurchasesNotSettled ShareBasedCompensation ShortTermBorrowings StateAndLocalIncomeTaxExpenseBenefitContinuingOperations StockholdersEquity StockIssued1 StockIssuedDuringPeriodValueAcquisitions TaxesOther TransferOtherRealEstate TransferToOtherRealEstate TreasuryStockValue TreasuryStockValueAcquiredCostMethod'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the target\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in word index: 24178\n"
     ]
    }
   ],
   "source": [
    "# generate tokenizer, fit to training data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train)\n",
    "\n",
    "# set up tag_index, which contains all of the tags and print the number of words\n",
    "tag_index = tokenizer.word_index\n",
    "print(\"Number of words in word index: \" + str(len(tag_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of tags:  158.8218111002921\n",
      "Max length of tags:  1486\n",
      "Median length of tags:  152.0\n"
     ]
    }
   ],
   "source": [
    "# find some statsistics on the number of tags\n",
    "average_number_of_tags = train.str.split().apply(len).mean()\n",
    "max_number_of_tags = train.str.split().apply(len).max()\n",
    "median_number_of_tags = train.str.split().apply(len).median()\n",
    "print(\"Average length of tags:  \" + str(average_number_of_tags))\n",
    "print(\"Max length of tags:  \" + str(max_number_of_tags))\n",
    "print(\"Median length of tags:  \" + str(median_number_of_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of sequences_padded: (1027, 300)\n"
     ]
    }
   ],
   "source": [
    "# setting variables for pad_sequences\n",
    "trunc_type='post'\n",
    "max_length = 300\n",
    "padding_type='post'\n",
    "\n",
    "# convert the text into sequences of numbers, then pad/trim those sequences to all be the same length\n",
    "sequences = tokenizer.texts_to_sequences(train)\n",
    "sequences_padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print(\"Shape of sequences_padded: \" + str(sequences_padded.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing ML Models\n",
    "Next, I will try out a few ML models and tune the hyperparameters using RandomizedSearch. Ultimately, XGBoost performed the best, but it still only achieved results of around 97% or 98%. This just isn't good enough to ensure that I will have all of the correct answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test set to use for testing some of the ML models after using RandomizedSearch\n",
    "train_df = pd.DataFrame(sequences_padded)\n",
    "y = pd.DataFrame(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df, y, test_size=0.33, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RandomizedSearchCV on XGBoost\n",
    "RandomizedSearchCV implements a fit and score. The paremeters of the estimator randomly searched based upon the param_grid that I specify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an XGBClassifier\n",
    "clf = XGBClassifier()\n",
    "\n",
    "# specifying the parameters to randomly search\n",
    "param_grid = {\n",
    "        'max_depth': [6, 10, 15, 20],\n",
    "        'learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],\n",
    "        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n",
    "        'gamma': [0, 0.25, 0.5, 1.0],\n",
    "        'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n",
    "        'n_estimators': [50, 100, 150, 200, 400]}\n",
    "\n",
    "# creating RandomizedSearchCV object\n",
    "rs_clf = RandomizedSearchCV(clf, param_grid, n_iter=250,\n",
    "                            n_jobs=1, \n",
    "                            verbose=1, cv=2,\n",
    "                            scoring='accuracy', refit=False, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized search..\n",
      "Fitting 2 folds for each of 250 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomized search time: 114.22986054420471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  1.9min finished\n"
     ]
    }
   ],
   "source": [
    "# fitting RandomizedSearchCV\n",
    "print(\"Randomized search..\")\n",
    "search_time_start = time.time()\n",
    "rs_clf.fit(train_df,np.ravel(y))\n",
    "print(\"Randomized search time:\", time.time() - search_time_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.9844187316540378\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Best params: \n",
      "colsample_bylevel: 1.0\n",
      "colsample_bytree: 0.5\n",
      "gamma: 0\n",
      "learning_rate: 0.2\n",
      "max_depth: 10\n",
      "min_child_weight: 1.0\n",
      "n_estimators: 50\n",
      "reg_lambda: 1.0\n",
      "subsample: 1.0\n"
     ]
    }
   ],
   "source": [
    "# taking a look at the best score and the best parameters\n",
    "best_score = rs_clf.best_score_\n",
    "best_params = rs_clf.best_params_\n",
    "print(\"Best score: {}\".format(best_score))\n",
    "print(\"--\" * 50)\n",
    "print(\"Best params: \")\n",
    "for param_name in sorted(best_params.keys()):\n",
    "    print('%s: %r' % (param_name, best_params[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       149\n",
      "           1       0.98      0.99      0.99       190\n",
      "\n",
      "    accuracy                           0.99       339\n",
      "   macro avg       0.99      0.98      0.99       339\n",
      "weighted avg       0.99      0.99      0.99       339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inputting the best parameters into an XGBClassifier, fitting on a portion of the training data, and testing on the cv set\n",
    "xgb = XGBClassifier(colsample_bylevel= 1.0,\n",
    "colsample_bytree= 0.5,\n",
    "gamma= 0,\n",
    "learning_rate= 0.2,\n",
    "max_depth= 10,\n",
    "min_child_weight= 1.0,\n",
    "n_estimators= 50,\n",
    "reg_lambda= 1.0,\n",
    "subsample=1.0)\n",
    "xgb.fit(X_train,np.ravel(y_train))\n",
    "predictions = xgb.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RandomizedSearchCV on Random Forest\n",
    "I also wanted to try out Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   47.7s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  6.2min finished\n",
      "C:\\Users\\lukem\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:765: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, estimator=RandomForestClassifier(), n_iter=100,\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000]},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(train_df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 600,\n",
       " 'min_samples_split': 5,\n",
       " 'min_samples_leaf': 1,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 60,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking a look at the best parameters\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94       149\n",
      "           1       0.94      0.96      0.95       190\n",
      "\n",
      "    accuracy                           0.94       339\n",
      "   macro avg       0.94      0.94      0.94       339\n",
      "weighted avg       0.94      0.94      0.94       339\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trying out the best parameters by creating a RandomForestClassifier and training it on part of the training data, then checking on the cv data\n",
    "random_forest = RandomForestClassifier(n_estimators=1800, min_samples_split=2, min_samples_leaf=1, max_features='auto',max_depth=20,bootstrap=False)\n",
    "random_forest.fit(X_train,np.ravel(y_train))\n",
    "predictions = random_forest.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Deep Learning Model\n",
    "The ML models are certainly performing well, getting 94%+ accuracy. However, this simply isn't good enough. I'm going to use a deep learning model to improve upon that even more. In order to check how my model is performing, I am going to use StratifiedShuffleSplit so that I can test out my model using 5 separate folds to see how well my model generalizes. After building my model, I found that I was achieving between 99.9% and 100% accuracy on each fold of the cv data. These results are much more promising, although the model is more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up variables\n",
    "embedding_dim = 22\n",
    "vocab_size = 24179"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to create the deep learning model\n",
    "def make_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n",
    "        tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "26/26 [==============================] - 3s 79ms/step - loss: 0.6891 - accuracy: 0.5279 - val_loss: 0.6356 - val_accuracy: 0.7524\n",
      "Epoch 2/4\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.5390 - accuracy: 0.8126 - val_loss: 0.1770 - val_accuracy: 1.0000\n",
      "Epoch 3/4\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.0877 - accuracy: 0.9983 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
      "Epoch 4/4\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.0059 - accuracy: 0.9997 - val_loss: 0.0040 - val_accuracy: 1.0000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1/4\n",
      "26/26 [==============================] - 1s 23ms/step - loss: 0.6898 - accuracy: 0.5793 - val_loss: 0.6425 - val_accuracy: 0.9515\n",
      "Epoch 2/4\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.5558 - accuracy: 0.9664 - val_loss: 0.2305 - val_accuracy: 0.9951\n",
      "Epoch 3/4\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1232 - accuracy: 0.9993 - val_loss: 0.0372 - val_accuracy: 0.9951\n",
      "Epoch 4/4\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0355 - val_accuracy: 0.9951\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1/4\n",
      "26/26 [==============================] - 1s 23ms/step - loss: 0.6866 - accuracy: 0.5388 - val_loss: 0.6300 - val_accuracy: 0.5437\n",
      "Epoch 2/4\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.5807 - accuracy: 0.6077 - val_loss: 0.3030 - val_accuracy: 0.9612\n",
      "Epoch 3/4\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.2264 - accuracy: 0.9922 - val_loss: 0.0250 - val_accuracy: 1.0000\n",
      "Epoch 4/4\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.0134 - accuracy: 0.9998 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1/4\n",
      "26/26 [==============================] - 1s 22ms/step - loss: 0.6853 - accuracy: 0.5432 - val_loss: 0.6171 - val_accuracy: 0.5874\n",
      "Epoch 2/4\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.5337 - accuracy: 0.7327 - val_loss: 0.2218 - val_accuracy: 0.9951\n",
      "Epoch 3/4\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.1373 - accuracy: 0.9982 - val_loss: 0.0122 - val_accuracy: 1.0000\n",
      "Epoch 4/4\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.0305 - accuracy: 0.9955 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1/4\n",
      "26/26 [==============================] - 1s 22ms/step - loss: 0.6850 - accuracy: 0.5685 - val_loss: 0.6122 - val_accuracy: 0.6699\n",
      "Epoch 2/4\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.5025 - accuracy: 0.8359 - val_loss: 0.1124 - val_accuracy: 1.0000\n",
      "Epoch 3/4\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.0605 - accuracy: 0.9985 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 4/4\n",
      "26/26 [==============================] - 0s 16ms/step - loss: 0.0048 - accuracy: 0.9995 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 4\n",
    "n_splits=5\n",
    "\n",
    "# create StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits = n_splits, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# train the neural network using each fold of the StratifiedShuffleSplit\n",
    "for train_index, test_index in sss.split(train_df, y):\n",
    "    X_train, X_test = train_df.iloc[train_index], train_df.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model = make_model()\n",
    "    model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_test, y_test), verbose=True)\n",
    "    print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "33/33 [==============================] - 1s 13ms/step - loss: 0.6823 - accuracy: 0.5218\n",
      "Epoch 2/4\n",
      "33/33 [==============================] - 0s 14ms/step - loss: 0.3732 - accuracy: 0.9167\n",
      "Epoch 3/4\n",
      "33/33 [==============================] - 0s 14ms/step - loss: 0.0202 - accuracy: 0.9976\n",
      "Epoch 4/4\n",
      "33/33 [==============================] - 0s 14ms/step - loss: 0.0034 - accuracy: 0.9998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21187a556d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model on the full dataset\n",
    "model = make_model()\n",
    "model.fit(sequences_padded, y, epochs=4, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using my Model to Predict the Test Data\n",
    "Now that I have created my model which performs really well, I'm going to use it to make the final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the tags to sequences of numbers\n",
    "test_sequences = tokenizer.texts_to_sequences(test)\n",
    "\n",
    "# pad/trim the sequences\n",
    "test_sequences_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "# make predictions\n",
    "predictions = model.predict(test_sequences_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the predictions ended up as a sigmoid activation. I will convert this to binary\n",
    "binary_predictions = []\n",
    "for item in predictions:\n",
    "    if item >= 0.5:\n",
    "        binary_predictions.append(1)\n",
    "    else:\n",
    "        binary_predictions.append(0)\n",
    "\n",
    "# convert to written labels\n",
    "binary_predictions_with_words = le.inverse_transform(binary_predictions)\n",
    "\n",
    "# create dataframe with the predictions\n",
    "bin_pred_df = pd.DataFrame(binary_predictions_with_words, columns=[\"Predictions\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export as a csv, removing the header and index\n",
    "bin_pred_df.to_csv('final_predictions_4_8_21.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Another Method to Predict the Data\n",
    "Now that I figured out how to achieve perfect results with a neural network, I want to see if I can find a way to get the same results but with a simpler model. To do this, I tried using a CountVectorizer instead of a tokenizer. Using this technique I was able to achieve 100% accuracy using a Naive Bayes model, without even needing to tune my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull in training set, split into train and and target\n",
    "train = pd.read_csv('financial-statement-ml-challenge-train.csv')\n",
    "y = train['FINANCIAL_STATEMENT_TEMPLATE']\n",
    "train = train['COMPANY_XBRL_TAGS']\n",
    "\n",
    "# pull in test set\n",
    "test = pd.read_csv('financial-statement-ml-challenge-test.csv')\n",
    "test = test['COMPANY_XBRL_TAGS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# fit and transform on the training data, transform the test data\n",
    "train = vectorizer.fit_transform(train)\n",
    "test = vectorizer.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and validation data for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple model without tuning any hyperparameters\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the ML model and make predictions\n",
    "y_pred = gnb.fit(X_train.toarray(), y_train).predict(X_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         fin       1.00      1.00      1.00       239\n",
      "        indu       1.00      1.00      1.00       275\n",
      "\n",
      "    accuracy                           1.00       514\n",
      "   macro avg       1.00      1.00      1.00       514\n",
      "weighted avg       1.00      1.00      1.00       514\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view results of predictions\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test set\n",
    "final_predictions = gnb.predict(test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fin', 'fin', 'indu', 'indu', 'indu', 'indu', 'fin', 'fin', 'fin',\n",
       "       'fin', 'fin', 'fin', 'indu', 'indu', 'indu', 'indu', 'indu', 'fin',\n",
       "       'indu', 'indu', 'fin', 'fin', 'fin', 'indu', 'indu', 'indu',\n",
       "       'indu', 'indu', 'indu', 'indu', 'indu', 'indu', 'indu', 'indu',\n",
       "       'fin', 'fin', 'fin', 'fin', 'indu', 'indu', 'fin', 'fin', 'fin',\n",
       "       'indu', 'indu', 'fin', 'fin', 'fin', 'fin', 'fin', 'fin'],\n",
       "      dtype='<U4')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export as a csv, removing the header and index\n",
    "pd.DataFrame(final_predictions).to_csv('final_predictions_nb_5_25_21.csv', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
